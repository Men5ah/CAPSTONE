{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d05acc",
   "metadata": {},
   "source": [
    "Training script for credential stuffing attack detection using temporal user login data.\n",
    "Includes EDA, preprocessing, feature importance, and training of RNN, LSTM, ConvLSTM models.\n",
    "Validation and evaluation with confusion matrices and metrics table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8386e3ec",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5926e682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 10:47:04.291323: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-23 10:47:05.407610: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745405225.725667  769497 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745405225.775347  769497 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745405226.702014  769497 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745405226.702054  769497 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745405226.702056  769497 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745405226.702058  769497 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-23 10:47:06.889816: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score, precision_score,\n",
    "                             recall_score, f1_score, roc_auc_score, classification_report)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, ConvLSTM2D, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf62c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to datasets\n",
    "# DATA_DIR = os.path.join(\"../Dataset\")\n",
    "CLEAN_TRAIN_PATH = 'clean_train.csv'\n",
    "NOISY_TRAIN_PATH = 'noisy_train.csv'\n",
    "CLEAN_TEST_PATH = 'clean_test.csv'\n",
    "NOISY_TEST_PATH = 'noisy_test.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0517f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(\"Loading datasets...\")\n",
    "    clean_train = pd.read_csv(CLEAN_TRAIN_PATH)\n",
    "    noisy_train = pd.read_csv(NOISY_TRAIN_PATH)\n",
    "    clean_test = pd.read_csv(CLEAN_TEST_PATH)\n",
    "    noisy_test = pd.read_csv(NOISY_TEST_PATH)\n",
    "    print(\"Datasets loaded.\")\n",
    "    return clean_train, noisy_train, clean_test, noisy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94db0edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eda(df, name):\n",
    "    print(f\"\\n--- EDA for {name} ---\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(\"Missing values:\\n\", df.isnull().sum())\n",
    "    print(\"Data types:\\n\", df.dtypes)\n",
    "    print(\"Basic statistics:\\n\", df.describe())\n",
    "    print(\"Class distribution:\\n\", df['is_bot'].value_counts(normalize=True))\n",
    "    \n",
    "    # Plot class distribution\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.countplot(x='is_bot', data=df)\n",
    "    plt.title(f'Class Distribution in {name}')\n",
    "    plt.savefig(f'{name}_class_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot feature distributions for numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols.remove('is_bot') if 'is_bot' in numeric_cols else None\n",
    "    for col in numeric_cols:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.histplot(df[col].dropna(), kde=True, bins=30)\n",
    "        plt.title(f'Distribution of {col} in {name}')\n",
    "        plt.savefig(f'{name}_{col}_distribution.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    plt.figure(figsize=(10,8))\n",
    "    df_corr = df.copy()\n",
    "    if 'user_id' in df_corr.columns:\n",
    "        df_corr = df_corr.drop(columns=['user_id'])\n",
    "    corr = df_corr.corr()\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "    plt.title(f'Correlation Heatmap for {name}')\n",
    "    plt.savefig(f'{name}_correlation_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"--- EDA for {name} completed ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "032db6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Impute missing values with median\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    features = df.drop(columns=['bot'])\n",
    "    features_imputed = pd.DataFrame(imputer.fit_transform(features), columns=features.columns)\n",
    "    \n",
    "    # Feature engineering: example - extract hour from timestamp if exists\n",
    "    if 'timestamp' in features_imputed.columns:\n",
    "        try:\n",
    "            features_imputed['timestamp'] = pd.to_datetime(features_imputed['timestamp'])\n",
    "            features_imputed['hour'] = features_imputed['timestamp'].dt.hour\n",
    "            features_imputed['dayofweek'] = features_imputed['timestamp'].dt.dayofweek\n",
    "            features_imputed['is_weekend'] = features_imputed['dayofweek'].isin([5,6]).astype(int)\n",
    "            features_imputed = features_imputed.drop(columns=['timestamp'])\n",
    "        except Exception as e:\n",
    "            print(f\"Timestamp feature engineering skipped due to error: {e}\")\n",
    "    \n",
    "    # Scaling features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = pd.DataFrame(scaler.fit_transform(features_imputed), columns=features_imputed.columns)\n",
    "    \n",
    "    labels = df['bot'].values\n",
    "    \n",
    "    return features_scaled, labels, imputer, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a1e9dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_rf(X, y, n_features=10):\n",
    "    print(\"\\nTraining Random Forest for feature importance...\")\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    importances = rf.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "    print(\"Top features:\\n\", feat_imp.head(n_features))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(x=feat_imp.head(n_features), y=feat_imp.head(n_features).index)\n",
    "    plt.title(\"Top Feature Importances\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"feature_importance.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    top_features = feat_imp.head(n_features).index.tolist()\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7172f2bf",
   "metadata": {},
   "source": [
    "## Sequence Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "822b0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, seq_length=10):\n",
    "    \"\"\"\n",
    "    Create sequences of features and labels for RNN/LSTM/ConvLSTM.\n",
    "    X: pd.DataFrame or np.array of shape (samples, features)\n",
    "    y: np.array of shape (samples,)\n",
    "    Returns:\n",
    "        X_seq: np.array of shape (num_sequences, seq_length, num_features)\n",
    "        y_seq: np.array of shape (num_sequences,)\n",
    "    \"\"\"\n",
    "    X_values = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(X_values) - seq_length + 1):\n",
    "        seq = X_values[i:i+seq_length]\n",
    "        label = y[i+seq_length-1]  # label corresponds to last item in sequence\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return np.array(sequences), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "500cbdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_convlstm_sequences(X, y, seq_length=10):\n",
    "    \"\"\"\n",
    "    Prepare data for ConvLSTM2D which expects 5D input: (samples, time_steps, rows, cols, channels)\n",
    "    We will reshape features into a 2D grid if possible.\n",
    "    For simplicity, reshape features into (1, num_features, 1) grid.\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = create_sequences(X, y, seq_length)\n",
    "    # Reshape to (samples, time_steps, rows=1, cols=num_features, channels=1)\n",
    "    X_seq_reshaped = X_seq.reshape((X_seq.shape[0], X_seq.shape[1], 1, X_seq.shape[2], 1))\n",
    "    return X_seq_reshaped, y_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04271f8",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a7879f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(64, activation='relu', input_shape=input_shape, return_sequences=False))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e61f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, activation='tanh', input_shape=input_shape, return_sequences=False))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53160e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_convlstm(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=input_shape, return_sequences=False))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0fe73",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c331d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, dataset_name, model_name):\n",
    "    print(f\"\\nEvaluating {model_name} on {dataset_name} dataset...\")\n",
    "    y_pred_prob = model.predict(X).ravel()\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    prec = precision_score(y, y_pred, zero_division=0)\n",
    "    rec = recall_score(y, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y, y_pred, zero_division=0)\n",
    "    roc_auc = roc_auc_score(y, y_pred_prob)\n",
    "    \n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    return {'model': model_name, 'dataset': dataset_name, 'confusion_matrix': cm,\n",
    "            'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'roc_auc': roc_auc}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a0bab",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "295e32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load data\n",
    "    clean_train, noisy_train, clean_test, noisy_test = load_data()\n",
    "    \n",
    "    # Perform EDA\n",
    "    perform_eda(clean_train, \"clean_train\")\n",
    "    perform_eda(noisy_train, \"noisy_train\")\n",
    "    \n",
    "    # Preprocess data\n",
    "    X_clean_train, y_clean_train, imputer_clean, scaler_clean = preprocess_data(clean_train)\n",
    "    X_noisy_train, y_noisy_train, imputer_noisy, scaler_noisy = preprocess_data(noisy_train)\n",
    "    \n",
    "    X_clean_test = clean_test.drop(columns=['bot'])\n",
    "    y_clean_test = clean_test['bot'].values\n",
    "    X_clean_test = pd.DataFrame(imputer_clean.transform(X_clean_test), columns=X_clean_test.columns)\n",
    "    X_clean_test = pd.DataFrame(scaler_clean.transform(X_clean_test), columns=X_clean_test.columns)\n",
    "    \n",
    "    X_noisy_test = noisy_test.drop(columns=['bot'])\n",
    "    y_noisy_test = noisy_test['bot'].values\n",
    "    X_noisy_test = pd.DataFrame(imputer_noisy.transform(X_noisy_test), columns=X_noisy_test.columns)\n",
    "    X_noisy_test = pd.DataFrame(scaler_noisy.transform(X_noisy_test), columns=X_noisy_test.columns)\n",
    "    \n",
    "    # Feature importance with Random Forest on clean and noisy train sets\n",
    "    top_features_clean = feature_importance_rf(X_clean_train, y_clean_train, n_features=10)\n",
    "    top_features_noisy = feature_importance_rf(X_noisy_train, y_noisy_train, n_features=10)\n",
    "    \n",
    "    # For modeling, use intersection of top features from both datasets to ensure consistency\n",
    "    selected_features = list(set(top_features_clean).intersection(set(top_features_noisy)))\n",
    "    if len(selected_features) < 5:\n",
    "        # If intersection too small, use union instead\n",
    "        selected_features = list(set(top_features_clean).union(set(top_features_noisy)))\n",
    "    print(f\"\\nSelected features for modeling: {selected_features}\")\n",
    "    \n",
    "    # Prepare sequences for models\n",
    "    seq_length = 10\n",
    "    \n",
    "    # Clean dataset sequences\n",
    "    X_clean_train_seq, y_clean_train_seq = create_sequences(X_clean_train[selected_features], y_clean_train, seq_length)\n",
    "    X_clean_test_seq, y_clean_test_seq = create_sequences(X_clean_test[selected_features], y_clean_test, seq_length)\n",
    "    \n",
    "    # Noisy dataset sequences\n",
    "    X_noisy_train_seq, y_noisy_train_seq = create_sequences(X_noisy_train[selected_features], y_noisy_train, seq_length)\n",
    "    X_noisy_test_seq, y_noisy_test_seq = create_sequences(X_noisy_test[selected_features], y_noisy_test, seq_length)\n",
    "    \n",
    "    # ConvLSTM requires 5D input\n",
    "    X_clean_train_conv, _ = create_convlstm_sequences(X_clean_train[selected_features], y_clean_train, seq_length)\n",
    "    X_clean_test_conv, _ = create_convlstm_sequences(X_clean_test[selected_features], y_clean_test, seq_length)\n",
    "    X_noisy_train_conv, _ = create_convlstm_sequences(X_noisy_train[selected_features], y_noisy_train, seq_length)\n",
    "    X_noisy_test_conv, _ = create_convlstm_sequences(X_noisy_test[selected_features], y_noisy_test, seq_length)\n",
    "    \n",
    "    # Build models\n",
    "    rnn_model_clean = build_rnn((seq_length, len(selected_features)))\n",
    "    lstm_model_clean = build_lstm((seq_length, len(selected_features)))\n",
    "    convlstm_model_clean = build_convlstm((seq_length, 1, len(selected_features), 1))\n",
    "    \n",
    "    rnn_model_noisy = build_rnn((seq_length, len(selected_features)))\n",
    "    lstm_model_noisy = build_lstm((seq_length, len(selected_features)))\n",
    "    convlstm_model_noisy = build_convlstm((seq_length, 1, len(selected_features), 1))\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    # Train models on clean dataset\n",
    "    print(\"\\nTraining models on clean dataset...\")\n",
    "    rnn_model_clean.fit(X_clean_train_seq, y_clean_train_seq, epochs=30, batch_size=64,\n",
    "                        validation_data=(X_clean_test_seq, y_clean_test_seq), callbacks=[early_stop], verbose=2)\n",
    "    lstm_model_clean.fit(X_clean_train_seq, y_clean_train_seq, epochs=30, batch_size=64,\n",
    "                        validation_data=(X_clean_test_seq, y_clean_test_seq), callbacks=[early_stop], verbose=2)\n",
    "    convlstm_model_clean.fit(X_clean_train_conv, y_clean_train_seq, epochs=30, batch_size=64,\n",
    "                            validation_data=(X_clean_test_conv, y_clean_test_seq), callbacks=[early_stop], verbose=2)\n",
    "    \n",
    "    # Train models on noisy dataset\n",
    "    print(\"\\nTraining models on noisy dataset...\")\n",
    "    rnn_model_noisy.fit(X_noisy_train_seq, y_noisy_train_seq, epochs=30, batch_size=64,\n",
    "                       validation_data=(X_noisy_test_seq, y_noisy_test_seq), callbacks=[early_stop], verbose=2)\n",
    "    lstm_model_noisy.fit(X_noisy_train_seq, y_noisy_train_seq, epochs=30, batch_size=64,\n",
    "                       validation_data=(X_noisy_test_seq, y_noisy_test_seq), callbacks=[early_stop], verbose=2)\n",
    "    convlstm_model_noisy.fit(X_noisy_train_conv, y_noisy_train_seq, epochs=30, batch_size=64,\n",
    "                           validation_data=(X_noisy_test_conv, y_noisy_test_seq), callbacks=[early_stop], verbose=2)\n",
    "    \n",
    "    # Evaluate models and generate confusion matrices\n",
    "    results = []\n",
    "    # Clean dataset evaluations\n",
    "    results.append(evaluate_model(rnn_model_clean, X_clean_test_seq, y_clean_test_seq, \"clean\", \"RNN\"))\n",
    "    results.append(evaluate_model(lstm_model_clean, X_clean_test_seq, y_clean_test_seq, \"clean\", \"LSTM\"))\n",
    "    results.append(evaluate_model(convlstm_model_clean, X_clean_test_conv, y_clean_test_seq, \"clean\", \"ConvLSTM\"))\n",
    "    \n",
    "    # Noisy dataset evaluations\n",
    "    results.append(evaluate_model(rnn_model_noisy, X_noisy_test_seq, y_noisy_test_seq, \"noisy\", \"RNN\"))\n",
    "    results.append(evaluate_model(lstm_model_noisy, X_noisy_test_seq, y_noisy_test_seq, \"noisy\", \"LSTM\"))\n",
    "    results.append(evaluate_model(convlstm_model_noisy, X_noisy_test_conv, y_noisy_test_seq, \"noisy\", \"ConvLSTM\"))\n",
    "    \n",
    "    # Create results table for noisy dataset\n",
    "    noisy_results = [r for r in results if r['dataset'] == 'noisy']\n",
    "    results_df = pd.DataFrame(noisy_results)[['model', 'accuracy', 'precision', 'recall', 'f1', 'roc_auc']]\n",
    "    print(\"\\nNoisy Dataset Results Summary:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Save results table to CSV\n",
    "    results_df.to_csv(\"noisy_dataset_results_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c408c04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Datasets loaded.\n",
      "\n",
      "--- EDA for clean_train ---\n",
      "Shape: (25000, 14)\n",
      "Columns: ['user_id', 'login_attempts', 'failed_logins', 'unusual_time_access', 'ip_rep_score', 'browser_type', 'new_device_login', 'session_duration_deviation', 'network_packet_size_variance', 'mouse_speed', 'typing_speed', 'day_of_week', 'time_of_day', 'is_bot']\n",
      "Missing values:\n",
      " user_id                         0\n",
      "login_attempts                  0\n",
      "failed_logins                   0\n",
      "unusual_time_access             0\n",
      "ip_rep_score                    0\n",
      "browser_type                    0\n",
      "new_device_login                0\n",
      "session_duration_deviation      0\n",
      "network_packet_size_variance    0\n",
      "mouse_speed                     0\n",
      "typing_speed                    0\n",
      "day_of_week                     0\n",
      "time_of_day                     0\n",
      "is_bot                          0\n",
      "dtype: int64\n",
      "Data types:\n",
      " user_id                          object\n",
      "login_attempts                    int64\n",
      "failed_logins                     int64\n",
      "unusual_time_access                bool\n",
      "ip_rep_score                    float64\n",
      "browser_type                     object\n",
      "new_device_login                   bool\n",
      "session_duration_deviation      float64\n",
      "network_packet_size_variance    float64\n",
      "mouse_speed                     float64\n",
      "typing_speed                    float64\n",
      "day_of_week                      object\n",
      "time_of_day                      object\n",
      "is_bot                             bool\n",
      "dtype: object\n",
      "Basic statistics:\n",
      "        login_attempts  failed_logins  ip_rep_score  \\\n",
      "count    25000.000000   25000.000000   25000.00000   \n",
      "mean         4.300800       1.887640       0.47117   \n",
      "std          2.258983       1.495508       0.28594   \n",
      "min          1.000000       0.000000       0.00000   \n",
      "25%          3.000000       1.000000       0.23000   \n",
      "50%          4.000000       2.000000       0.45000   \n",
      "75%          5.000000       3.000000       0.72000   \n",
      "max          9.000000       5.000000       1.00000   \n",
      "\n",
      "       session_duration_deviation  network_packet_size_variance   mouse_speed  \\\n",
      "count                25000.000000                  25000.000000  25000.000000   \n",
      "mean                     2.110991                      1.169217      2.251331   \n",
      "std                      1.440538                      0.863789      1.299858   \n",
      "min                      0.000000                      0.000000      0.500000   \n",
      "25%                      0.890000                      0.460000      1.180000   \n",
      "50%                      1.800000                      0.900000      1.850000   \n",
      "75%                      3.330000                      1.880000      3.300000   \n",
      "max                      5.000000                      3.000000      5.000000   \n",
      "\n",
      "       typing_speed  \n",
      "count  25000.000000  \n",
      "mean      44.410528  \n",
      "std       20.218503  \n",
      "min       10.010000  \n",
      "25%       26.867500  \n",
      "50%       43.960000  \n",
      "75%       62.000000  \n",
      "max       80.000000  \n",
      "Class distribution:\n",
      " is_bot\n",
      "False    0.55548\n",
      "True     0.44452\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Firefox'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m clean_train, noisy_train, clean_test, noisy_test = load_data()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Perform EDA\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mperform_eda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclean_train\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m perform_eda(noisy_train, \u001b[33m\"\u001b[39m\u001b[33mnoisy_train\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Preprocess data\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mperform_eda\u001b[39m\u001b[34m(df, name)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df_corr.columns:\n\u001b[32m     31\u001b[39m     df_corr = df_corr.drop(columns=[\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m corr = \u001b[43mdf_corr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcorr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m sns.heatmap(corr, annot=\u001b[38;5;28;01mTrue\u001b[39;00m, fmt=\u001b[33m\"\u001b[39m\u001b[33m.2f\u001b[39m\u001b[33m\"\u001b[39m, cmap=\u001b[33m'\u001b[39m\u001b[33mcoolwarm\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     34\u001b[39m plt.title(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCorrelation Heatmap for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env1/lib64/python3.12/site-packages/pandas/core/frame.py:11049\u001b[39m, in \u001b[36mDataFrame.corr\u001b[39m\u001b[34m(self, method, min_periods, numeric_only)\u001b[39m\n\u001b[32m  11047\u001b[39m cols = data.columns\n\u001b[32m  11048\u001b[39m idx = cols.copy()\n\u001b[32m> \u001b[39m\u001b[32m11049\u001b[39m mat = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m  11051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mpearson\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m  11052\u001b[39m     correl = libalgos.nancorr(mat, minp=min_periods)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env1/lib64/python3.12/site-packages/pandas/core/frame.py:1993\u001b[39m, in \u001b[36mDataFrame.to_numpy\u001b[39m\u001b[34m(self, dtype, copy, na_value)\u001b[39m\n\u001b[32m   1991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1992\u001b[39m     dtype = np.dtype(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1993\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype:\n\u001b[32m   1995\u001b[39m     result = np.asarray(result, dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env1/lib64/python3.12/site-packages/pandas/core/internals/managers.py:1694\u001b[39m, in \u001b[36mBlockManager.as_array\u001b[39m\u001b[34m(self, dtype, copy, na_value)\u001b[39m\n\u001b[32m   1692\u001b[39m         arr.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m     arr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[32m   1696\u001b[39m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env1/lib64/python3.12/site-packages/pandas/core/internals/managers.py:1753\u001b[39m, in \u001b[36mBlockManager._interleave\u001b[39m\u001b[34m(self, dtype, na_value)\u001b[39m\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1752\u001b[39m         arr = blk.get_values(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1753\u001b[39m     \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m = arr\n\u001b[32m   1754\u001b[39m     itemmask[rl.indexer] = \u001b[32m1\u001b[39m\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m itemmask.all():\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'Firefox'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
